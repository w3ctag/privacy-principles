# TAG Privacy TF Minutes - Wed, 15 September 2021 

## Metadata

* Location: https://whereby.com/privacy-tf
* Time: https://www.timeanddate.com/worldclock/fixedtime.html?iso=20210915T16
* Chair: Jeffrey
* Scribes: Robin
* Attendees:
   * Don
   * Jonathan
   * [your name here]
* Regrets:
   * Dan
   * Tess
   * Amy
   * Wendy

## Agenda

PRs:
* [People & Data style improvements](https://github.com/w3ctag/privacy-principles/pull/30), by Robin
* Parties ü•≥üéäüéÅ: [#32](https://github.com/w3ctag/privacy-principles/pull/32), [#43](https://github.com/w3ctag/privacy-principles/pull/43), [#44](https://github.com/w3ctag/privacy-principles/pull/44), by Don and Jeffrey 
* [Unexpected profiling](https://github.com/w3ctag/privacy-principles/pull/38), by Pete
* [Cross-context recognition](https://github.com/w3ctag/privacy-principles/pull/40), by Jeffrey

[Issues](https://github.com/w3ctag/privacy-principles/issues):
* ???

## Minutes

### [People & Data style improvements](https://github.com/w3ctag/privacy-principles/pull/30), by Robin

JY: Is there anything to look at?

RB: I'll go through this over the next week.

### Parties ü•≥üéäüéÅ: [#32](https://github.com/w3ctag/privacy-principles/pull/32), [#43](https://github.com/w3ctag/privacy-principles/pull/43), [#44](https://github.com/w3ctag/privacy-principles/pull/44), by Don and Jeffrey 

JY: I wrote PR44 yesterday, trying to capture our discussion from last week moving away from a legal definition. [reads] I tried to write examples of
refinements and failed. How do people like this?

CR: Thank you for attempting the impossible! I will need to think about it a little bit more. I need to think about how it might work in practice.
We won't know if we're happy before we try to use this.

JY: The example I came up was Don's suggestion from FPS. I suspect domains will place different restrictions on this than other approaches.

CR: What do we mean by domain?

JY: Something that we try to apply to apply these principles to, eg. for FPS purposes or a particular company's website.

CR: Is domain the same as use?

JY: ‚Ä¶probably‚Ä¶? It's when you try to apply these principles to a specific question.

CR: Domain gets used differently in the Web space, I'd like to make sure we're clear.

RB reads a bunch of synonyms

JY & CR: I don't hate "sphere."

CR: I don't care much about the world, I just want to make sure this is abundantly clear.

JY: Let's take a week to figure that out.


### [Unexpected profiling](https://github.com/w3ctag/privacy-principles/pull/38), by Pete

DM: I'm looking at this from the POV of real-world sites. Eg. a poll and you want to make sure that people can't vote twice, or soft paywalls in which
you only get a few articles before you have to subscribe. Recognising people is technically profiling them, but I want to be cautious that we would have
a principle that comes into conflict with things that sites feel they have a reasonable reason to do.

CR: I think that we landed on "unwanted profiling." If the site is clear on why it is doing some such thing is different from a site that is doing
something in an obscure way. Inferring things about the user when they visit a site are different.

PS: I am not sure that these are good use cases. We should define privacy independently of what sites decide they need.

DM: I don't know what to think about the use case of sites knowing if I'm a returning user. I get wigged out. From the user POV, it's a concern, from
the site POV it's value.

CR: I just want to be clear to Pete's point that I want to make sure that our design principles provide the best experience to the user. In terms of this
document, we are defining the problem space. How it gets resolved may be outside the scope of this document.

RB: I lean toward Christine: this might be difficult to resolve in this document. Don't think privacy is defined by having control over my own data. It doesn't work that way in the analog world. In a shop, a soft paywall, say "1 free sample", it's just that the shop owner recognizes you, and if you come back in a week they might not. That interaction would be ideal in a digital context as well, but we might not be able to reproduce that.

JY: I think Don's example is a good one to keep in mind, not because it should be in the document, but the doc's principles should lead to an answer one
way or another. This soft recognition that "I have seen you before but I don't necessarily know who you are" is something that we should keep in mind. Some
mechanisms can be excluded but the use case doesn't have to be. There is also the issue of fraud prevention that intersects with this too.

DM: There's the example of sites that give away things, one per user, eg. a tenth of a Bitcoin.

JY: Is this enough of an answer to Don's question in the issue?

CR: I think so, but I would like to think about whether "I've seen you before" could cause a privacy harm. I have a sneaking feeling that it could cause
harm. There may be edge cases, I need to think about it.

DM: there's the case of sites targeted at a sensitive group, and people could be recognised in a context that would be detrimental to a user (for example, if a user has a personal account and then tries to make an advertiser account for work and is denied).

RB: You can think of domestic violence shelter websites as a case where it shouldn't ever be clear that you are a returning user.

PS: Does this help?

DM: I can see both sides and I don't know what the answer is.

PS: Let's not treat this as blocking until we have resolution.

RB: [thumbs up]

JY: I was talking to people inside Google about this question. The case of Target knowing that people were pregnant before they were ready to share. Once Target
stopped acting on the information, did that remove the privacy harm?

CR: I think it's still a problem.

RB: Yes, there is a privacy violation and there is harm from the action. These can be decorrelated, but the violation persists if there is no action.

CR: The action could happen later, eg. if they get hacked.

JY: There is risk even if there is no action.

CR: A violation is a harm. Rights have been breached.

DM: The mail deliverer will see who is likely pregnant, even if the dad doesn't see it.

RB: There is the further issue that Target might not have inferred pregnancy, but they have the data from which it could be inferred.

JY: Having an order history is somethign that users expect, too. 

RB: And often the law requires you to keep an order history, for example for accounting.

CR: Suppose that hte user wants you to keep the information, and you have a legal requirement. How the information is stored and who has access, is often not thought through enough. There's a spectrum of things one can do to make it harder. E.g. maybe only the user should have access? There's a whole research project in itself.

RB: And there's the issue of concentration. Ok if Target has your Target orders, Walmart has their orders, but collecting all the orders together can be a problem.

JY: My question came from the non-digital version of this. A human observes you for a while, then makes an inference. Is that inference a
privacy harm because you didn't expect it, or is it using that inference?

CR: The challenge with doing human vs computer is that these two things work in very different ways.

RB: And people do inferences without meaning to. If we're at a party and you're not drinking, I might infer that you're pregnant even without meaning to, and without being able to avoid it. 

CR: You're also an individual at a party, and privacy law hasn't figured out how to deal with an individual being the data collector.

JY: In most cases, we recommend trying to figure out the non-digital version as an intuition pump but in this case the two are actually quite different so it's interesting that it doesn't work.

RB: Yes and no. The digital correlates, to figuring out you're pregnant from behavior at a party, would be in a chat room, vs a shopkeeping figuring it out.

JY: I'd like to capture this.

PS: The ambiguity is deliberate.

JY: It's important to note that not everything is sensitive to everyone.

### [Cross-context recognition](https://github.com/w3ctag/privacy-principles/pull/40), by Jeffrey

JY: Last week we got to the trade-off discussion.

CR topics
```
Discuss: could we avoid using the ‚Äútrade-off‚Äù terminology?
Re: ‚Äúand users have come to expect this behavior,‚Äù add "in such contexts" because user's would not always have that expectation.
Re: "When a violation occurs at their divisions between different browser profiles or at the point a user clears their cookies and site storage .." consider adding other signals such as use of extensions or add-ons to blocking tracking and private or incognito mode.
Re: "The web platform offers many ways for a website to recognize that a user is using the same identity over time .." - does this include ways for a website to recognise that a user has changed identity but is the same user?
Discuss: "A privacy harm occurs if the user expresses that they intend ..". I understand the point of trying to tie the privacy harm to something that has been positively communicated by the user, but a privacy harm can occur if what happens does not match a user's expectation (whether or not they actively expressed t
```

CR: We got to the point where Jeffrey had used "trade-offs" but there isn't necessarily a loss which is implied here. Could we use a different term?
In some cases it may be true but I'm not sure that's the case across the board.

JY: My understanding is that there's a Pareto frontier and most initial designs are not there. Once you're there, however, you are making trade-offs and
discussing which is best is a good discussion to have.

CR: We hear a lot that you need to lose privacy in order to do X. I just want to be mindful that we're not inadvertently reinforcing that.

RB: I wonder if Jeffrey's Pareto discussion captures the nuance. One proposes something that's not on the Pareto frontier, we redesign it over to the frontier, and then move along it. I agree with Christine that when people say "if we have privacy, forget about invalid-traffic detection, etc." it's often not true. But there are also cases where the tradeoffs are real.

DM: There are positive spinoffs of privacy things as well. E.g. cross-context tracking makes it less feasible to advertise on low-trust or low-engagement sites, driving more budgets to higher-trust, higher-engagement sites. Producing higher volumes of material for those users as a side-effect of privacy. When we give up things for privacy, we can also mention that the privacy protection can produce additional value for users.

RB: Or for companies.

JY: Trying to think about what to change in the text. 

CR: Maybe we can say that the privacy threat model will explain how to address both things. Or balance or integrate or a word that doesn't imply that somebody is going
to lose.

JY: We should be explicit that in a lot of cases you can get both.

CR: Yes.

CR: Next is editorial, `and users have come to expect this behavior` could be qualified.

JY: I'm fine with `in some contexts`.

CR: So long as there's some sort of qualifier that doesn't make it about all cases.

JY: "In some contexts" it is.

CR: Next is a suggestion to add some examples of other signals such as extensions or Incognito.

JY discusses text variations

CR: What is the difference between a division and a partition?

JY: A partition is space in which the user has the same identity. We draw boundaries between these (eg. site boundaries, browser profile boundaries). You're right that
we should add something about extensions and the such.

CR: Question about recognising that a user has changed identity but is the same user?

JY: We don't want to have that.

CR: Good.

JY: Not sure what to change though, but this also ties to Don's earlier question about recognising that a user has been there before.

CR: I want to make sure that a privacy harm can occur whether or not a user has explicitly expressed their intent, that moves us into consent territory.

JY: This distinguishes between what contexts might exist inside a site and what the browser can differentiate. If the site presents as if there are
multiple identities but the site joins them. It's true that this could happen by deceptive UI rather than user expression.

CR: It makes me uncomfortable to link the user expressing something with privacy harm.

JY: When I see 'harm', I infer that someone's to blame

CR: 'intentional' or 'malicious' privacy harm? But we don't necessarily want to limit it to malicious harm.

JY: I worry about unreasonable expectations.

RB: We could say "reasonable expectations" if that's the concern.

CR: This has a defined meaning.

RB: Yes, jurisdictions often have reasonable person tests.

CR: Next I had a question about "UA responsibility" but I can't recall what bothered me. I think the problem was that we made the UA responsibility pretty
limited. We should call out that irrespective of the limited responsibility, the UA can still do specific things to protect users.

JY: What do they do?

JK: Brave used to use an allow list but it broken things. We use CNAME and IP addresses to recognises known trackers.

JY: The harm there is that information that should be 1P gets leaked to people who shouldn't know it.

JK: There is also the "more evil" case of CNAME pretending to be 1P.

JY: This is not quite same-site recognition, it's 1P information leaking. 

CR: If the 1P is relying on GA, does it relate to same-site recognition.

JY: This probably goes somewhere else but is useful.




